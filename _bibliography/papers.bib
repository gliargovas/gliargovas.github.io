@inproceedings{liargkovas2025speculative_actions,
  title     = {Speculative Actions: A Lossless Framework for Faster Agentic Systems},
  author    = {Liargkovas, Georgios and Ye, Naimeng and Ahuja, Arnav and Lu, Yunan and Kaffes, Kostis and Peng, Tianyi},
  booktitle = {Proceedings of the 2025 International Conference on Learning Representations (ICLR '25)},
  year      = {2025},
  address   = {},
  publisher = {Under Review},
  abbr      = {Under Review},
  pdf       = {/assets/pdf/Liargkovas_Spec_Actions.pdf},
  bibtex_show = {true},
  selected  = {true},
  abstract  = {Despite growing interest in AI agents across industry and academia, their execution in an environment is often slow, hampering training, evaluation, and deployment. Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, we propose speculative actions, a lossless framework that predicts likely actions using faster models, enabling multiple steps to be executed in parallel. We evaluate this framework across four agentic environments: gaming, e-commerce, web search, and operating systems. In all cases, speculative actions achieve substantial accuracy in next-action prediction, translating into significant reductions in end-to-end latency.},
}

@inproceedings{liargkovas2025llm_tuning,
  title     = {An Expert in Residence: LLM Agents for Always-On Operating System Tuning},
  author    = {Liargkovas, Georgios and Jabrayilov, Vahab and Franke, Hubertus and Kaffes, Kostis},
  booktitle = {ML for Systems Workshop at NeurIPS},
  year      = {2025},
  url       = {https://mlforsystems.org/},
  note      = {To appear},
  selected={true},
  abbr={ML4Sys '25},
  pdf={/assets/pdf/Liargkovas_ML4Sys_NeurIPS25.pdf},
  bibtex_show={true},
  doi={},
  abstract={Classical machine-learning auto-tuners for OS control struggle with semantic gaps, brittle rewards, and unsafe exploration. 
We introduce an online, LLM-driven agent that emulates expert reasoning for continuous OS optimization. 
When tuning the Linux Completely Fair Scheduler’s hyperparameters, the agent outperforms Bayesian optimization by 5\% in single-parameter tuning, 7.1\% in two-parameter co-tuning, and a human expert by 2.98\% overall, while converging faster and adapting more quickly to workload changes. 
When application counters are unavailable, system-level proxies (e.g., Instructions Per Cycle (IPC)) preserved tail latency in our setup.
Putting this together, we propose adopting the Model Context Protocol (MCP) for tool/resource discovery and invocation and a logging channel; on top of that, we propose adding transactional apply--commit--revert, host-mediated approval gates, and policy controls in the OS-tuning server and host to ensure safe, auditable operation. Our results and the proposed architecture point toward a new generation of self-adapting, expert-level OS tuners.}
},

@inproceedings{liargkovas2025proxy_tuning,
  title     = {Set It and Forget It: Zero-Mod ML Magic for Linux Tuning},
  author    = {Liargkovas, Georgios and Sodhi, Prabhpreet Singh and Kaffes, Kostis},
  booktitle = {Proceedings of the 2025 ACM Workshop on Practical Adoption Challenges of ML for Systems (PACMI '25)},
  year      = {2025},
  pages     = {1--7},
  address   = {Seoul, Republic of Korea},
  publisher = {ACM},
  abbr={PACMI '25},
  url={https://dl.acm.org/doi/10.1145/3766882.3767175},
  doi       = {10.1145/3766882.3767175},
  pdf={/assets/pdf/Liargkovas_PACMI_25.pdf},
  bibtex_show={true},
  selected={true},
  doi={10.1145/3766882.3767175},
  abstract={Machine learning can turbocharge OS optimization---if one is willing to reinvent the whole stack. Recent work pushes exotic instrumentation or new OS designs that break real-world constraints, demanding app metrics nobody can (or wants to) provide. 
The alternative---naively optimizing for simple system proxies like IPC---is just as flawed, leading to misleading results that fail to generalize across real-world workloads.
Our framework sidesteps this dilemma by learning to optimize without direct visibility. 
Instead of building brittle models to predict absolute performance, we reframe the problem to learn the relative ranking of system configurations, using a diversified performance signature built from the system counters the OS already has. 
The outcome is a scalable, robust, and ML-driven performance boost for real applications---delivered without demanding radical shifts in the OS landscape.},
}

@inproceedings{10.1145/3748355.3748363,
author = {Sodhi, Prabhpreet Singh and Liargkovas, Georgios and Kaffes, Kostis},
title = {Empowering machine-learning assisted kernel decisions with eBPFML},
year = {2025},
isbn = {9798400720840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3748355.3748363},
doi = {10.1145/3748355.3748363},
abstract = {Machine-learning (ML) techniques can optimize core operating system paths---scheduling, I/O, power, and memory---yet practical deployments remain rare. Existing prototypes either (i) bake simple heuristics directly into the kernel or (ii) off-load inference to user space to exploit discrete accelerators, both of which incur unacceptable engineering or latency cost. We argue that eBPF, the Linux kernel's safe, hot-swappable byte-code runtime, is the missing substrate for moderately complex in-kernel ML. We present eBPFML, a design that (1) extends the eBPF instruction set with matrix-multiply helpers, (2) leverages upcoming CPU matrix engines such as Intel Advanced Matrix Extensions (AMX) through the eBPF JIT, and (3) retains verifier guarantees and CO-RE portability.},
journal = {Proceedings of the 3rd Workshop on EBPF and Kernel Extensions},
pages = {28–30},
numpages = {3},
keywords = {Operating systems, eBPF, hardware acceleration, machine learning},
location = {Coimbra, Portugal},
series = {eBPF '25},
selected={true},
abbr={eBPF '25},
bibtex_show={true},
doi={10.1145/3748355.3748363},
pdf={https://dl.acm.org/doi/10.1145/3748355.3748363},
}


@INPROCEEDINGS{LKGV23,
  author={Liargkovas, Georgios and Kallas, Konstantinos and Greenberg, Michael and Nikos Vasilakis},
  journal={The 19th Workshop on Hot Topics in Operating Systems}, 
  title={Executing Shell Scripts in the Wrong Order, Correctly}, 
  year={2023},
  selected={true},
  abbr={HotOS '23},
  doi={10.1145/3593856.3595891},
  bibtex_show={true},
  pdf={https://sigops.org/s/conferences/hotos/2023/papers/liargkovas.pdf},
  abstract={Shell scripts are critical infrastructure for developers, administrators, and scientists; and ought to enjoy the performance
benefits of the full suite of advances in compiler optimizations. But between the shell’s inherent challenges and neglect
from the community, shell tooling and performance lags far
behind the state of the art. We propose executing scripts
out-of-order to better use modern computational resources.
Optimizing any part of an arbitrary shell script is very challenging: the shell language’s complex, late-bound semantics
makes extensive use of opaque external commands with
arbitrary side effects.
We work with the grain of the shell’s challenges, meeting
dynamism with dynamism: we optimize at runtime, speculatively executing commands in an isolated and monitored
environment to determine and contain their behavior. Our
proposed approach can yield serious performance benefits
(up to 3.9x for a bioinformatics script on a 16-core machine)
for arbitrarily complex scripts without modifying their behavior. Contained out-of-order execution obviates the need
for command specifications, operates on external commands,
and yields a much more general framework for the shell.
Script writers need not change a thing and observe no differences: they get improved performance with the interpretability of sequential output.}
}


@ARTICLE{9612087,
  author={Liargkovas, Georgios and Papadopoulou, Angeliki and Kotti, Zoe and Spinellis, Diomidis},
  journal={IEEE Transactions on Education}, 
  title={Software Engineering Education Knowledge Versus Industrial Needs}, 
  year={2022},
  volume={65},
  number={3},
  pages={419-427},
  doi={10.1109/TE.2021.3123889},
  selected={false},
  abbr={TE '22},
  pdf={https://arxiv.org/pdf/2112.12834.pdf},
  bibtex_show={true},
}


@ARTICLE{liargkovas2023quieting,
  title={Quieting the Static: A Study of Static Analysis Alert Suppressions},
  author={Liargkovas, Georgios and Panourgia, Evangelia and Spinellis, Diomidis},
  journal={arXiv preprint},
  year={2023},
  abbr={arxiv},
  doi={https://doi.org/10.48550/arXiv.2311.07482},
  pdf={https://arxiv.org/pdf/2311.07482.pdf},
  selected={false},
  bibtex_show={true}
}

